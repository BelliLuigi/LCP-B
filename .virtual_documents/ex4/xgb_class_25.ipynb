


import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
# AdaBoost Algorithm
from sklearn.ensemble import AdaBoostClassifier
# Gradient Boosting 
from sklearn.ensemble import GradientBoostingClassifier
# XGBoost 
import xgboost
from xgboost import XGBClassifier
from xgboost import plot_importance, to_graphviz, plot_tree
print("XGBoost version:",xgboost.__version__)

mycmap = "winter"
mpl.rcParams['image.cmap'] = mycmap
plt.rcParams['font.size'] = 13


import os
import json
from keras.optimizers import SGD
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from matplotlib.ticker import MultipleLocator
from sklearn.preprocessing import StandardScaler

#rng = np.random.default_rng(1758)






def Standardize(x):
    """
    Rescales each component using its mean and standard deviation
    """
    x_mean = np.mean(x, axis=0)
    x_std = np.std(x,axis=0)
    N = len(x)
    # assuming len(m)=len(s)=len(x[0])
    mm,ss = np.tile(x_mean,(N,1)), np.tile(x_std,(N,1))
    return (x-mm)/ss


np.random.seed(12345)

dname="./DATA/"
str0="_XGB_25.dat"
fnamex=dname+'x'+str0
fnamey=dname+'y'+str0
x = np.loadtxt(fnamex, delimiter=" ",dtype=float)
y = np.loadtxt(fnamey)
x = Standardize(x)
y = y.astype(int)
N,L = len(x), len(x[0])

N_train = int(0.75*N)
x_train,y_train = x[:N_train],y[:N_train]
x_test,y_test = x[N_train:],y[N_train:]
print(f"N={N}, N_train={N_train}, L={L}")





def scat(ax,x,y,i=0,j=1,s=4,title=""):
    ax.scatter(x[:,i],x[:,j],s=s,c=y)
    ax.set_xlabel(f"feature {i}")
    ax.set_ylabel(f"feature {j}")
    ax.set_title(title)

for n in range(5):
    print(x[n],y[n])

fig,AX = plt.subplots(2,2,figsize=(8.5,8.1))
scat(AX[0,0],x_train,y_train,title="Train")
scat(AX[0,1],x_train,y_train,i=2,j=3,title="Train")
scat(AX[1,0],x_test,y_test,title="Test")
scat(AX[1,1],x_test,y_test,i=2,j=3,title="Test")
fig.tight_layout()
plt.show()


def classify(clf=GradientBoostingClassifier(),show=True):
    # GradientBoostingClassifier():
    #   n_estimators = 100 (default)
    #   loss function = deviance(default) used in Logistic Regression
    # XGBClassifier()
    #   n_estimators = 100 (default)
    #   max_depth = 3 (default?)
    clf.fit(x_train,y_train)
    y_hat = clf.predict(x_test)
    
    print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-clf.score(x_test, y_test)),clf.score(x_test, y_test)))

    
    S=2 ### Watch out
    dx = 0.02 ## also this
    x_seq=np.arange(-S,S+dx,dx)
    nx = len(x_seq)
    x_plot=np.zeros((nx*nx,L))
    q=0
    for i in range(nx):
        for j in range(nx):
            x_plot[q,:2] = [x_seq[i],x_seq[j]]
            q+=1
    y_plot= clf.predict(x_plot)

    fig,AX = plt.subplots(1,2,figsize=(8.2,4))
    scat(AX[0],x_plot[:],y_plot,s=2,title="predicted")
    scat(AX[1],x_train[:],y_train,s=4,title="training set")
    fig.tight_layout()
    plt.show()
    
    if show:      
        dump_list = clf.get_booster().get_dump()
        num_trees = len(dump_list)
        print("num_trees=",num_trees)
        
        fig, AX = plt.subplots(2,1,figsize=(12, 5))
        for i in range(min(2,num_trees)):
            ax=AX[i]
            plot_tree(clf, num_trees=i, ax=ax)
        fig.savefig("DATA/tree-classif.png", dpi=400, pad_inches=0.02)   
        plt.show()
    return clf.score(x_test, y_test)











lista_lr=[]



#change manually here
n_estimators=44
reg_lambda=1e-4

#Here we explore different values of the learning rate.
for learning_rate in range(1,17):
    model = XGBClassifier(seed=1,
                        objective='binary:logistic',
                        importance_type="gain", #weight, cover, ...
                        learning_rate=learning_rate/10,
                        reg_lambda=reg_lambda, 
                        n_estimators=n_estimators)
    
    model.fit(x_train,y_train)
    
    print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-model.score(x_test, y_test)),model.score(x_test, y_test)))
    a=model.score(x_test,y_test)
    lista_lr.append((learning_rate/10,a)) 
00000	


lr=np.array(lista_lr)
plt.scatter(lr[:,0],lr[:,1])
stringa='Learning rate, with reg_lambda='+str(reg_lambda)+', n_estimators='+str(n_estimators)
plt.title(stringa)
#plt.savefig('lr_1e-4_44')


with open('data_lr.txt',mode='a') as f:
    print(reg_lambda,n_estimators,file=f)
    print(lr,file=f)



lista_rl=[]


#change manually here
learning_rate=0.4
n_estimators=44

reg_lambdas=np.logspace(-9,1,num=11)
for reg_lambda in reg_lambdas:
    model = XGBClassifier(seed=1,
                        objective='binary:logistic',
                        importance_type="gain",
                        learning_rate=learning_rate,
                        reg_lambda=reg_lambda, 
                        n_estimators=n_estimators)

    model.fit(x_train,y_train)
    print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-model.score(x_test, y_test)),model.score(x_test, y_test)))
    a=model.score(x_test,y_test)
    lista_rl.append((reg_lambda,a)) 


print(lista_rl)
rl=np.array(lista_rl)
plt.xscale('log')
plt.scatter(rl[:,0],rl[:,1])

stringa='Reg. lambda, with learning_rate='+str(learning_rate)+', n_estimators='+str(n_estimators)
plt.title(stringa)

plt.savefig('rl_4e-1_44_b')


with open('data_rl.txt',mode='w') as f:
    print(learning_rate,n_estimators,file=f)
    print(rl,file=f)


lista_ne=[]


#change manually here
learning_rate=0.4
reg_lambda=1e-4

#n_estimators=30
for n_estimators in range(2,51,2):
    model = XGBClassifier(seed=1,
                        objective='binary:logistic',
                        importance_type="gain", #weight, cover, ...
                        learning_rate=learning_rate,
                        reg_lambda=reg_lambda, 
                        n_estimators=n_estimators)

    model.fit(x_train,y_train)
    print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-model.score(x_test, y_test)),model.score(x_test, y_test)))
    a=model.score(x_test,y_test)
    lista_ne.append((n_estimators,a)) 


print(lista_ne)
ne=np.array(lista_ne)
plt.scatter(ne[:,0],ne[:,1])
stringa='N. estimators, with learning_rate='+str(learning_rate)+', reg. lambda='+str(reg_lambda)
plt.title(stringa)

plt.savefig('ne_4e-1_1e4')


with open('data_ne.txt',mode='w') as f:
    print(learning_rate,reg_lambda,file=f)
    print(ne,file=f)





scores_reg=[]
scores=[]
x=range(2,51,2)
for n_estimators in x:
    model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain", #weight, cover, ...
                    learning_rate=0.4,
                    reg_lambda=1e-4, 
                    n_estimators=n_estimators)
    model.fit(x_train,y_train)
    sc_reg=model.score(x_test, y_test)
    scores_reg.append(sc_reg)

    model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain", #weight, cover, ...
                    learning_rate=0.4,
                    reg_lambda=None, 
                    n_estimators=n_estimators)
    model.fit(x_train,y_train)
    sc=model.score(x_test, y_test)
    scores.append(sc)


plt.title('Accuracies comparison')
plt.grid(True,alpha=0.3, zorder=2.03)

plt.scatter(x,scores,c='k',label='w/o regularization', zorder=2.01)
plt.scatter(x,scores_reg,c='r',label='with regularization', zorder=2.02)
plt.legend(loc='lower right')
plt.xlabel('n_estimators')
plt.ylabel('Accuracies')
#plt.savefig('accuracies comparison')






learning_rate=0.4
n_estimators=2
reg_lambda=None

simple_model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain",
                    learning_rate=learning_rate,
                    reg_lambda=reg_lambda, 
                    n_estimators=n_estimators)

simple_model.fit(x_train,y_train)
y_hat = simple_model.predict(x_test)
    
#print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-simple_model.score(x_test, y_test)),simple_model.score(x_test, y_test)))
classify(clf=simple_model,show=False)





learning_rate=0.4
n_estimators=44
reg_lambda=1e-4

best_model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain",
                    learning_rate=learning_rate,
                    reg_lambda=reg_lambda, 
                    n_estimators=n_estimators)
classify(clf=best_model,show=False)



# https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/
# feature importance
print(best_model.importance_type)
print(best_model.feature_importances_)

# plot
my_cmap = plt.get_cmap("Reds")
rescale = lambda y: 0.3 + 0.7 * (y - np.min(y)) / (np.max(y) - np.min(y))
plt.bar(range(len(best_model.feature_importances_)), best_model.feature_importances_,
        color=my_cmap(rescale(best_model.feature_importances_)))
plt.xlabel("feature")
plt.ylabel("importance")
plt.xticks(np.arange(L))
plt.title(best_model.importance_type)
plt.show()

rescale_r = lambda y: 0.3 + 0.7 * (np.max(y) - y) / (np.max(y) - np.min(y))
plot_importance(best_model,color=my_cmap(rescale_r(best_model.feature_importances_)))
plt.show()





# if the importance type is “total_gain”, then the score is sum of loss change for each split from all trees.
list_type=["weight","gain","total_gain","cover","total_cover"]
list_col=["#7565F0","k","#666666","#F0A000","gold"]

for i,t in enumerate(list_type):
    feature_imp = best_model.get_booster().get_score(importance_type=t)
    keys = list(feature_imp.keys())
    values = np.array(list(feature_imp.values()))
    print(i,t,values)
    values= values/np.sum(values)
    plt.bar(np.arange(L)+(i-L/2)/10, values,color=list_col[i],width=0.1,label=t)
plt.xlabel("feature")
plt.ylabel("importance (normalized to 1)")
plt.xticks(np.arange(L))
plt.legend()
plt.show()





dname="./DATA/"
str0="_XGB_25.dat"
fnamex=dname+'x'+str0
fnamey=dname+'y'+str0
x = np.loadtxt(fnamex, delimiter=" ",dtype=float)
y = np.loadtxt(fnamey)
y = y.astype(int)

# REMOVING ONE FEATURE (feature 2)
x=np.delete(x,2,axis=1)

N,L = len(x), len(x[0])

N_train = int(0.75*N)
x_train,y_train = x[:N_train],y[:N_train]
x_test,y_test = x[N_train:],y[N_train:]
print(f"N={N}, N_train={N_train}, L={L}")



learning_rate=0.4
n_estimators=44
reg_lambda=1e-4

reduced_best_model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain",
                    learning_rate=learning_rate,
                    reg_lambda=reg_lambda, 
                    n_estimators=n_estimators)
#classify(clf=reduced_best_model,show=False)

reduced_best_model.fit(x_train,y_train)
y_hat = reduced_best_model.predict(x_test)
    
print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-reduced_best_model.score(x_test, y_test)),reduced_best_model.score(x_test, y_test)))


# https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/
# feature importance
print(reduced_best_model.importance_type)
print(reduced_best_model.feature_importances_)

# plot
my_cmap = plt.get_cmap("Reds")
rescale = lambda y: 0.3 + 0.7 * (y - np.min(y)) / (np.max(y) - np.min(y))
plt.bar(range(len(reduced_best_model.feature_importances_)), reduced_best_model.feature_importances_,
        color=my_cmap(rescale(reduced_best_model.feature_importances_)))
plt.xlabel("feature")
plt.ylabel("importance")
plt.xticks(np.arange(L))
plt.title(reduced_best_model.importance_type)
plt.show()

rescale_r = lambda y: 0.3 + 0.7 * (np.max(y) - y) / (np.max(y) - np.min(y))
plot_importance(reduced_best_model,color=my_cmap(rescale_r(reduced_best_model.feature_importances_)))
plt.show()


# if the importance type is “total_gain”, then the score is sum of loss change for each split from all trees.
list_type=["weight","gain","total_gain","cover","total_cover"]
list_col=["#7565F0","k","#666666","#F0A000","gold"]

for i,t in enumerate(list_type):
    feature_imp = reduced_best_model.get_booster().get_score(importance_type=t)
    keys = list(feature_imp.keys())
    values = np.array(list(feature_imp.values()))
    print(i,t,values)
    values= values/np.sum(values)
    plt.bar(np.arange(L)+(i-L/2)/10, values,color=list_col[i],width=0.1,label=t)
plt.xlabel("feature")
plt.ylabel("importance (normalized to 1)")
plt.xticks(np.arange(L))
plt.legend()
plt.show()


learning_rate=0.4
n_estimators=2
reg_lambda=None

reduced_simple_model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain",
                    learning_rate=learning_rate,
                    reg_lambda=reg_lambda, 
                    n_estimators=n_estimators)

reduced_simple_model.fit(x_train,y_train)
y_hat = reduced_simple_model.predict(x_test)
    
print("errors: {:.2f}%   Accuracy={:.3f}".format(100*(1-reduced_simple_model.score(x_test, y_test)),reduced_simple_model.score(x_test, y_test)))


# https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/
# feature importance
print(reduced_simple_model.importance_type)
print(reduced_simple_model.feature_importances_)

# plot
my_cmap = plt.get_cmap("Reds")
rescale = lambda y: 0.3 + 0.7 * (y - np.min(y)) / (np.max(y) - np.min(y))
plt.bar(range(len(reduced_simple_model.feature_importances_)), reduced_simple_model.feature_importances_,
        color=my_cmap(rescale(reduced_simple_model.feature_importances_)))
plt.xlabel("feature")
plt.ylabel("importance")
plt.xticks(np.arange(L))
plt.title(reduced_simple_model.importance_type)
plt.show()

rescale_r = lambda y: 0.3 + 0.7 * (np.max(y) - y) / (np.max(y) - np.min(y))
plot_importance(reduced_simple_model,color=my_cmap(rescale_r(reduced_simple_model.feature_importances_)))
plt.show()


# if the importance type is “total_gain”, then the score is sum of loss change for each split from all trees.
list_type=["weight","gain","total_gain","cover","total_cover"]
list_col=["#7565F0","k","#666666","#F0A000","gold"]

for i,t in enumerate(list_type):
    feature_imp = reduced_simple_model.get_booster().get_score(importance_type=t)
    keys = list(feature_imp.keys())
    values = np.array(list(feature_imp.values()))
    print(i,t,values)
    values= values/np.sum(values)
    plt.bar(np.arange(L-1)+(i-(L-1)/2)/10, values,color=list_col[i],width=0.1,label=t)
plt.xlabel("feature")
plt.ylabel("importance (normalized to 1)")
plt.xticks(np.arange(L))
plt.legend()
plt.show()





np.random.seed(12345)

dname="./DATA/"
str0="_XGB_25.dat"
fnamex=dname+'x'+str0
fnamey=dname+'y'+str0
x = np.loadtxt(fnamex, delimiter=" ",dtype=float)
y = np.loadtxt(fnamey)
x = Standardize(x)
y = y.astype(int)
N,L = len(x), len(x[0])

N_train = int(0.75*N)
x_train,y_train = x[:N_train],y[:N_train]
x_test,y_test = x[N_train:],y[N_train:]
print(f"N={N}, N_train={N_train}, L={L}")


#########Original Model##############
from keras.optimizers import SGD
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from numpy.random import Generator, PCG64

rng = Generator(PCG64(42))  # usa il bit generator PCG64 con seed 42

def build_model():
    model = Sequential()
    L = 3 #num features
    
    # Input layer
    model.add(Dense(L, input_shape=(L,), activation='relu'))
    hidd_active = 'elu'
    
    # 3 hidden layers
    for layer_num in range(1, 4):
        model.add(Dense(20, activation=hidd_active))
        if layer_num !=3:
            model.add(Dropout(0.0))
        else:
            model.add(Dropout(0.2))

    # Output layer
    model.add(Dense(1,activation='sigmoid'))
    learning_rate = 0.1
    hyper_optimizer = 'RMSprop'
        
    model.compile(loss='binary_crossentropy', optimizer=hyper_optimizer, metrics=["accuracy"])
    return model


#########Improved model############
from keras.optimizers import Adam
from keras.layers import BatchNormalization

def build_model_improved():
    model = Sequential()
    L = 4  # features

    #hidden layers
    model.add(Dense(60, input_shape=(L,), activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))

    model.add(Dense(60, activation='relu'))
    #model.add(BatchNormalization())
    model.add(Dropout(0.1))

    model.add(Dense(30, activation='relu'))
    model.add(Dropout(0.1))

    #output
    model.add(Dense(1, activation='sigmoid'))

    optimizer = Adam(learning_rate=0.005)

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model


best_model = build_model_improved()
fit = best_model.fit(x_train, y_train, batch_size = 50, validation_data=(x_test,y_test), verbose=0,) # Training best model and evaluating it using the test dataset


def k_fold_cross_validation(x_train, y_train, k):

    idx = rng.permutation(np.arange(x_train.shape[0]))
    x_folds = np.array_split(x_train[idx], k)
    y_folds = np.array_split(y_train[idx], k)

    best_model = None
    best_perf = -1
    model_perf = []

    for test in range(k):
        x_val = x_folds[test]
        y_val = y_folds[test]
        x_train_fold = np.concatenate([x for i, x in enumerate(x_folds) if i != test])
        y_train_fold = np.concatenate([y for i, y in enumerate(y_folds) if i != test])

        model = build_model_improved()
        model.fit(x_train_fold,y_train_fold,epochs=200,verbose=0,validation_data=(x_val, y_val),callbacks=[EarlyStopping(patience=10, restore_best_weights=False)],)

        score = model.evaluate(x_val, y_val, verbose=0)[1]
        model_perf.append(score)

        if score > best_perf:
            best_perf = score
            best_model = model

    results = (np.mean(model_perf), np.std(model_perf))
    return best_model, results


best_model, results = k_fold_cross_validation(x_train, y_train, 5)
print("Cross-validation results (mean, std):", results)


#####CV per NN
x_range = [round(0.9 ** i, 3) for i in range(1, 8)]
NN_accuracies_values = []
for fraction in x_range[::-1]:
    num_samples = int(len(x_train) * fraction)
    sample_idx = np.random.choice(len(x_train), size=num_samples, replace=False)
    sample_x_train, sample_y_train = x_train[sample_idx],y_train[sample_idx]
    _, results = k_fold_cross_validation(sample_x_train, sample_y_train, 5)

    NN_accuracies_values.append(results[0])
    
    print("Cross-validation results (mean, std):", results)


learning_rate=0.4
n_estimators=44
reg_lambda=1e-4
xgb_best_model = XGBClassifier(seed=1,
                    objective='binary:logistic',
                    importance_type="gain",
                    learning_rate=learning_rate,
                    reg_lambda=reg_lambda, 
                    n_estimators=n_estimators)
accuracy_values = []

for fraction in np.arange(1,8):
    num_samples = int(len(x_train) * 0.9)
    sample_idx = np.random.choice(len(x_train), size=num_samples, replace=False)
    x_train, y_train = x_train[sample_idx],y_train[sample_idx]
    accuracy_values.append(classify(clf=xgb_best_model,show=False))



x_range = [round(0.9 ** i, 3) for i in range(1, 8)]

plt.plot(x_range, accuracy_values, marker='o', label='xgboost')
plt.plot(x_range, NN_accuracies_values, marker='s', label='FFNN')
plt.xlabel('Fraction of training set')
plt.ylabel('Validation accuracy')
plt.legend()
plt.show()
plt.close()



