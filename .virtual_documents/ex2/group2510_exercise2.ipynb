





import sys, datetime
import os

from numba import njit, prange
from helper import plot_weights_bias, show_MNIST, MNIST_bit

import numpy as np
from numpy import exp,sqrt,log,log10,sign,power,cosh,sinh,tanh,floor
rng = np.random.default_rng(12345)
np.set_printoptions(precision=4)

import matplotlib as mpl
get_ipython().run_line_magic("matplotlib", " inline")
from matplotlib.ticker import NullFormatter, MaxNLocator
mpl.rcParams.update({"font.size": 12})
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_openml





X_original, Y_original = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)





################################
#    CHOICE OF PARAMETERS      #
################################
# number of MNIST digits to keep (e.g., Ndigit=3 keeps 0,1,2)
Ndigit=3
# number of hidden units
L = 3 
# use (+1,-1) if SPINS, otherwise use bits (1,0), ising spins
SPINS=False  
# use one-hot encoding in hidden space if POTTS (use only if SPINS=False)
POTTS=False

dname='DATA/'
################################

# x_min =0 if bits, x_min=-1 if SPINS
if SPINS:
    x_min=-1
    level_gap=2.
else:
    x_min=0
    level_gap=1.

if POTTS:
    str_simul="RBM_Potts"
    # in one-hot encoding, number of possible hidden states matches L
    Nz=L
else:
    str_simul="RBM"
    # number of possible hidden states: 2**L
    Nz=2**L
    
if POTTS and SPINS: 
    print("\n\n>>>>>>>> WARNING:  POTTS and SPINS cannot coexist\n\n")





# Selecting digits to use
list_10_digits = ('0','1','2','3','4','5','6','7','8','9')
list_digits = list_10_digits[-Ndigit:] 
print('Chosen digits: ',list_digits)

# keep only X and Y in list_digitssssssssss
keep=np.isin(Y_original, list_digits)
X_keep,Y=X_original[keep],Y_original[keep]


data,label = MNIST_bit(X_keep, x_min = x_min),Y
data,label = data.astype("int"),label.astype("int")

# number of data points
Nd = len(data)
# number of visible units - I.e. total pixels of the image
D  = len(data[1])






print(f"Extract of MNIST-{Ndigit} data points, binarized")
for i in range(4): show_MNIST(data[i*20:],Nex=20)


def show_MNIST(x, y=[], z=[], Nex=5, S=1.4, side=0, colors=[], save=False):
    """Show digits"""
    if side==0: side = int(sqrt(x.shape[1]))
    if len(y)<1: y=np.full(Nex,"")
    colors=np.array(colors)
    fig, AX = plt.subplots(1,Nex,figsize=(S*Nex,S))
    
    for i, img in enumerate(x[:Nex].reshape(Nex, side, side)):
        if len(colors)==0: newcmp = "grey"
        else:
            col= colors[0] + (colors[1]-colors[0])*(i+1)/(Nex+1)
            newcmp = ListedColormap((col,(1,1,1,1)))
        ax=AX[i]
        ax.imshow(img, cmap=newcmp)
        ax.set_xticks([])
        ax.set_yticks([])
        if len(y)>0: ax.set_title(y[i])
        if len(z)>0: ax.set_title(''.join(map(str, z[i])),fontsize=9)
    if save: plt.savefig(f"./FIG/FRAME/RBM_{epoch}_w-a.png")
    plt.show()

def MNIST_bit(X,side=28,level=0.5):
    NX=len(X)
    print(f"Dataset with {NX} points, each with {len(X[0])} bits\n")
    if side==14:
        X = np.reshape(X,(NX,28,28))
        # new value = average over 4 values in a 2x2 square
        Xr = 0.25*(X[:,0::2,0::2]+X[:,1::2,0::2]+X[:,0::2,1::2]+X[:,1::2,1::2])
        X  = Xr.reshape(NX,side**2)

    # binarize data and then convert it to 1/0 or 1/-1
    X = np.where(X/255 > level, 1, x_min)
    return X.astype("int")


# Selecting digits to use
list_10_digits = ('0','1','2','3','4','5','6','7','8','9')
list_digits = list_10_digits[-Ndigit:] 
print('Chosen digits: ',list_digits)

# keep only X and Y in list_digitssssssssss
keep=np.isin(Y_original, list_digits)
X_keep,Y=X_original[keep],Y_original[keep]


data,label = MNIST_bit(X_keep),Y
data,label = data.astype("int"),label.astype("int")

# number of data points
Nd = len(data)
# number of visible units - I.e. total pixels of the image
D  = len(data[1])






# eq(213) page 97, activation via sigmoid
# taking into account energy gap 2 for "spin" variables (-1,1)
#@jit(parallel=True)
def CD_step(v_in,wei,bias,details=False,POTTS=False):
    """
        Generates the state on the other layer: 
        Field "H" ->  activation "a" -> probability "p" -> Spins/bits v_out

        Either (v_in=x, wei=w) or (v_in=z, wei=w.T)

        details = True --> returns also probability (p) and activation (a) 

        POTTS means one-hot encoding (used only in hidden space)
    """
    # local "field"
    H = np.clip(np.dot(v_in.astype(np.float64), wei) + bias, a_min=-300, a_max=300)
    # "activation" note, it is a sigmoid
    a = np.exp(level_gap*H)
    n = np.shape(H)
    v_out = np.full(n, x_min, dtype=int) # initially, just a list on -1's or 0's
    if POTTS: # RBM with a single hidden unit = 1 (that is, "one-hot encoding" with L states)
        # p: state probability, normalized to 1 over all units=states
        
        # Probability of turning on a hidden unit
        p = a/np.sum(a)
        # F: cumulative probability
        F = np.cumsum(p)
        # pick a state "i" randomly with probability p[i]
        r = np.random.rand()
        i = 0
        while r>F[i]:
            i+=1
        v_out[i] = 1 # activate a single hidden unit
    else: # normal Ising RBM
        # p: local probability, normalized to 1 for each hidden unit
        p = a / (a + 1.)
        # at each position i, activate the 1's with local probability p[i]
        v_out[np.random.random_sample(n) < p] = 1 

    if details: return (v_out,p,a)
    else: return v_out








# initial bias of visible units, based on their average value in the dataset
# Hinton, "A Practical Guide to Training Restricted Boltzmann Machines"
def Hinton_bias_init(x):
    '''
    x ::shape of 'x'(data): (21770,784)
    xmean :: does the mean on 'data' for each column -> shape of xmean :(,784)
     xmin =0, xmax=1 if SPINS=FALSE, otherwise {-1,+1}
    since np.precision is set at 4 -> S = 1e-4
    level_gap :: is the difference in values between the max (1) and the min (x_min)
    np.clip :: avoid minimum maximum overflow, inf and -inf values.
    '''
    xmean=np.array(np.mean(x,axis=0))
    # remove values at extrema, to avoid divergences in the log's
    S = 1e-4
    x1,x2 = x_min+S,1-S 
    xmean[xmean<x1] = x1
    xmean[xmean>x2] = x2
    return (1/level_gap)*np.clip(log(xmean-x_min) - log(1-xmean),-300,300)
    
# range of each initial weight
# Glorot and Bengio, "Understanding the difficulty of training deep feedforward neural networks"
sigma = sqrt(4. / float(L + D))








import numpy as np
import itertools as it

def generate_configurations(L):
    """Generate all binary configurations of length L."""
    return np.array(list(it.product((0,1), repeat=L)), dtype=np.int64)

def compute_log_likelihood(partition,data, a, b, w, configurations):
    log_likelihoods = np.zeros(len(data))  # Preallocate array
    #all_conf = generate_configurations(L)  # Compute binary configs once

    Hz_cache = np.array([H(a,w,z.astype(np.float64)) for z in configurations])  # Precompute H(z)

    for i, x in enumerate(data): 
        Z_x = 0.0
        for Hz, z in zip(Hz_cache, configurations):
            E_xz = -np.dot(Hz, x) - np.dot(b, z)
            Z_x += np.exp(-E_xz)

        log_likelihoods[i] = np.clip(np.log(Z_x) - partition, a_min=-700, a_max=+7000)

    return np.mean(log_likelihoods)

@njit(parallel=True)
def H(a,w,z):  
    return a + np.dot(w, z)  

@njit(parallel=True)
def G(b,z):  
    return np.prod(np.exp(b * z))

@njit(parallel=True)
def partition_function(a,w,b,configurations):
    sum_values = np.zeros(1, dtype=np.float64)#0.0  # Use scalar accumulation

    for i in prange(configurations.shape[0]):
        z = configurations[i]
        Hz = H(a,w,z.astype(np.float64))
        produttoria_H = np.prod(1 + np.exp(Hz))
        sum_values += G(b,z.astype(np.float64)) * produttoria_H

    return np.log(sum_values[0])








import time
## Hyper parameters::
L = 16
print(f'L, hidden units: {L}')
SPINS = False
print(f'SPINS: {SPINS}')
POTTS = False
print(f'POTTS: {POTTS}')
gamma = .001
print(f'gamma: {gamma}')
GRAD = 'RMSprop'
print(f'GRAD: {GRAD}')
Nt = 1
print(f'Nt: {Nt}')
Nepoch = 150
print(f'Epochs: {Nepoch}')
Nmini = 10
print(f'Nmini: {Nmini}')
N_ini = 5
print(f'Nini: {N_ini}')
N_fin = 10
print(f'Nfin: {N_fin}')


## setup for gradient choice & gamma
if GRAD=="SGD":
    l_rate_ini,l_rate_fin=1.0,0.25
if GRAD=="RMSprop":
    beta,epsilon=0.9,1e-4
    l_rate_ini,l_rate_fin=0.05,0.05
    print("epsilon=",epsilon)

print(f"D={D}\tsample size\nL={L}\tnr. z states")
print("Gradient descent type:",GRAD)
print(f"learning rate        = {l_rate_ini} --> {l_rate_fin}")
if gamma!=0: print(f"gamma={gamma}\tregularization")

##setup for spin/potts/other thing
if SPINS:
    x_min=-1
    level_gap=2.
else:
    x_min=0
    level_gap=1.

if POTTS:
    str_simul="RBM_Potts"
    Nz=L
else:
    str_simul="RBM"
    Nz=2**L
    
if POTTS and SPINS: 
    print("\n\n>>>>>>>> WARNING:  POTTS and SPINS cannot coexist\n\n")

stringa = f'#L SPIN POTTS gamma Nt Nepoch Nmini Nini Nfin Grad'
stringa2 = [L, SPINS, POTTS, gamma, Nt, Nepoch, Nmini, N_ini, N_fin, GRAD]

# Uncomment to save files
with open(f'llh{int((code := time.time()))}.txt', 'a') as add:
    print(stringa, file=add)
    print(stringa2, file=add)
    





for seed in [1, 12, 123, 1234, 12345, 123457]:
    np.random.seed(seed)
    
    # initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)
    w = sigma * np.random.randn(D,L)
    # using Hinton initialization of visible biases
    a = Hinton_bias_init(data)
    # hidden biases initialized to zero
    b = np.zeros(L)
    print(f"Nepoch={Nepoch}\nNmini={Nmini}")
    # recording history of weights ("E" means epoch)
    wE,aE,bE=np.zeros((Nepoch+1,D,L)),np.zeros((Nepoch+1,D)),np.zeros((Nepoch+1,L)) 
    wE[0],aE[0],bE[0]=np.copy(w),np.copy(a),np.copy(b)
    gwE,gw2E,gwE_d,gwE_m = np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE)
    gaE,ga2E,gaE_d,gaE_m = np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE)
    gbE,gb2E,gbE_d,gbE_m = np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE)
    miniE = np.zeros(Nepoch+1)
    pzE=np.zeros((Nepoch+1,Nz))
    
    if GRAD=="RMSprop": 
        gw2,ga2,gb2 = np.zeros_like(w),np.zeros_like(a),np.zeros_like(b)
    
    indices=np.arange(Nd).astype("int")
    #plot_weights_bias(wE, aE, 0, L, cols=L//2, save=False)
    
    # for the plot with panels
    Ncols=min(8,max(2,L//2))
    
    if POTTS: print("Starting the training, POTTS=True")
    else: print("Starting the training")
    #for L in np.arange(4,9,1):
    
    configurations = generate_configurations(L)
    log_likelihoods = []
    lista_epoch_grafico = []
    # Note: here an epoch does not analyze the whole dataset
    for epoch in range(1,1+Nepoch):
        # q maps epochs to interval [0,1]
        q = (epoch-1.)/(Nepoch-1.) 
        # N, size of the mini batch
        # stays closer to N_ini for some time, then it progressively accelerates toward N_fin
        N = int(N_ini + (N_fin-N_ini)*(q**2)) ## N increases w/epoch
        #  l_rate interpolates between initial and final value
        l_rate = l_rate_ini + (l_rate_fin-l_rate_ini)*q
    
        for mini in range(Nmini):
            # initializitation for averages in minibatch
            # visible variables "v" --> "x"
            #  hidden variables "h" --> "z"
            x_data, x_model = np.zeros(D),np.zeros(D)
            z_data, z_model = np.zeros(L),np.zeros(L)
            xz_data,xz_model= np.zeros((D,L)),np.zeros((D,L))
            pz = np.zeros(L)
    
            # Minibatch of size N: points randomply picked (without repetition) from data
            selected = np.random.choice(indices,N,replace=False)
           # if epoch==1 and mini<=3: print(selected)
    
            for k in range(N):
                ###################################
                x0 = data[selected[k]]
                #print('digit:', label[selected[k]])
                # positive CD phase: generating z from x[k]
                z = CD_step(x0,w,b,POTTS=POTTS) # just some z are 1 other are 0
                x_data  += x0
                z_data  += z
                #print(z_data)
                xz_data += np.outer(x0,z)
                # fantasy
                zf=np.copy(z)
                # Contrastive divergence with Nt steps
                for t in range(Nt):
                    # negative CD pzase: generating fantasy xf from fantasy zf
                    xf = CD_step(zf,w.T,a)
                    # positive CD phase: generating fantasy zf from fantasy xf 
                    zf = CD_step(xf,w,b,POTTS=POTTS)
                x_model += xf
                z_model += zf
                xz_model+= np.outer(xf,zf)
                # recording probability of encoding in z-space, if POTTS
                if POTTS: pz[zf]+=1
                ###################################
            
            # gradient of the likelihood: follow it along its positive direction
            gw_d,gw_m = xz_data/N, xz_model/N
            ga_d,ga_m = x_data/N, x_model/N
            gb_d,gb_m = z_data/N, z_model/N
            gw=np.copy(gw_d - gw_m)
            ga=np.copy(ga_d - ga_m)
            gb=np.copy(gb_d - gb_m)
    
            # gradient ascent step
            if GRAD=="RMSprop":
                # RMSprop gradient ascent
                gw2 = beta*gw2+(1-beta)*np.square(gw)
                ga2 = beta*ga2+(1-beta)*np.square(ga)
                gb2 = beta*gb2+(1-beta)*np.square(gb)
                w += l_rate*gw/sqrt(epsilon+gw2)
                a += l_rate*ga/sqrt(epsilon+ga2)
                b += l_rate*gb/sqrt(epsilon+gb2)
            else: 
                # defaulting to the vanilla stochastic gradient ascent (SGD)
                w += l_rate*gw
                a += l_rate*ga
                b += l_rate*gb
            # regularization (LASSO)
            if gamma>0.:
                w -= (gamma*l_rate)*sign(w)
                a -= (gamma*l_rate)*sign(a)
                b -= (gamma*l_rate)*sign(b)
        
        wE[epoch],gwE[epoch],gwE_d[epoch],gwE_m[epoch]=np.copy(w),np.copy(gw),np.copy(gw_d),np.copy(gw_m)
        aE[epoch],gaE[epoch],gaE_d[epoch],gaE_m[epoch]=np.copy(a),np.copy(ga),np.copy(ga_d),np.copy(ga_m)
        bE[epoch],gbE[epoch],gbE_d[epoch],gbE_m[epoch]=np.copy(b),np.copy(gb),np.copy(gb_d),np.copy(gb_m)
        miniE[epoch]=N
        if POTTS: pzE[epoch] = pz/np.sum(pz)
        #print("epoch",epoch,"/",Nepoch," Nt:",Nt," N:",N," L:",L,
             # " rate:",l_rate," gam:",gamma,"SPINS=",SPINS,"POTTS=",POTTS)
    
        #if Nepoch<=100 or epoch%20==0 or epoch==Nepoch:
            #plot_weights_bias(wE, aE, epoch, L, cols=Ncols, save=False)
    
        if epoch==(Nepoch):
            partition = partition_function(a,w,b,configurations)
            log_likelihood = compute_log_likelihood(partition,data, a=a, b=b, w=w,configurations=configurations)
            print(log_likelihood)
            # Uncomment to save files
            with open(f'llh{int(code)}.txt', 'a') as add:
                print(log_likelihood, file=add)
        str_time_completion = datetime.datetime.now().strftime("_%Y%m%d_%H%M")
    #w_not_zero = np.where(np.any(w != 0, axis=1))[0]
    #for idx in w_not_zero:
    #    print(f"Index: {idx}, Row: {w[idx]}")    
    print("END of learning phase")





llh_list = []
file_names = []
with os.scandir() as files:
    for file in files:
        if file.name.startswith('llh') and file.name.endswith('.txt'):
            llh_list.append(np.loadtxt(file.name, skiprows=2))
            file_names.append(file.name)
print(file_names)





fig, ax = plt.subplots(layout='constrained')
ax.errorbar(range(len(llh_list)), np.mean(llh_list, axis=0), yerr=np.std(llh_list, axis=0), fmt='ok', lw=1, capsize=2, label='Mean LL value and $\\sigma$')
plt.title('Best models Log-Likelihood Scores')
plt.ylabel('Log-likelihood scores')
plt.xlabel('Best models')
plt.legend()
plt.show()





ee=-1 ##
NN=200
traj_x,traj_z = np.zeros((NN+2,D)), np.zeros((NN+2,L))
xf=np.copy(data[np.random.randint(Nd)])
traj_x[0]=np.copy(xf)

# AF: multiply weights and biases by a number >1 to achieve a more deterministic behavior,
# equivalent to lower the temperature in a Boltzmann weight -> select lowest energies
# Note: here, this is done in the negative CD step only
AF=2

for t in range(NN):
    t1=t+1
    # positive CD phase: generating fantasy zf from fantasy xf
    zf = CD_step(xf,1*wE[ee],1*bE[ee],POTTS=POTTS)
    traj_z[t] = np.copy(zf)
    # negative CD pzase: generating fantasy xf from fantasy zf
    xf = CD_step(zf,AF*wE[ee].T,AF*aE[ee])
    traj_x[t1] = np.copy(xf)


plot_weights_bias(wE, aE, Nepoch, L, cols=Ncols, save=False)

col0,col1,col2,col3,col4=(0.8,0,0,1),(1,0.6,0,1),(0,.7,0,1),(0.2,0.2,1,1),(1,0,1,1)
show_MNIST(traj_x[0:],Nex=10,colors=(col0,col1))
show_MNIST(traj_x[10:],Nex=10,colors=(col1,col2))
show_MNIST(traj_x[20:],Nex=10,colors=(col2,col3))
show_MNIST(traj_x[40:],Nex=10,colors=(col3,col4))
print("L:",L,"    amplification of weights:",AF)





import os
from IPython.display import display
from PIL import Image
import matplotlib.pyplot as plt
import re

folder_path = "img/"
# regex
pattern = re.compile(r"LLH_epoch150_L(\d)_CD(\d+)_lr0.05.png")

files = [
    (f, int(match.group(1)), int(match.group(2)), os.path.join(folder_path, f))
    for f in os.listdir(folder_path)
    if (match := pattern.match(f)) 
]

# Sort by L first, then CD
files.sort(key=lambda x: (x[1], x[2]))
# Extract only filenames
sorted_filenames = [f[0] for f in files]

row,cols = 5,6
fig,axes = plt.subplots(row,cols, figsize=(15,10), layout='constrained', dpi=1000)
axes = axes.flatten()
for i, img in enumerate(sorted_filenames[:row*cols]):
    image = Image.open('img/' +img)
    axes[i].imshow(image)
    axes[i].axis('off')

plt.show()
