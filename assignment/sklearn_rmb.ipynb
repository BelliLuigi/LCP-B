{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e1c2c0-8b27-41aa-82e3-153470cd0a3a",
   "metadata": {},
   "source": [
    "# Sklearn RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7968c3-6f38-4f91-bd2c-786de069e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘llh_fig’: File exists\n",
      "mkdir: cannot create directory ‘weights’: File exists\n",
      "mkdir: cannot create directory ‘lr_fig’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir llh_fig\n",
    "!mkdir weights\n",
    "!mkdir lr_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbaa7c7-d0f6-48c7-8d04-e0117df18cf9",
   "metadata": {},
   "source": [
    "# RBM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49a0b647-9ec6-4957-bb24-373681f68dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import tensorflow as tf\n",
    "from scipy.special import expit\n",
    "\n",
    "def Hinton_bias_init(x, x_min=0, level_gap=1):\n",
    "    '''\n",
    "    Initialize visible biases based on the average value in the dataset.\n",
    "    Hinton, \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "    x: data array\n",
    "    x_min: minimum value of x\n",
    "    level_gap: difference between max and min values in x\n",
    "    '''\n",
    "    xmean = np.mean(x, axis=0)\n",
    "    S = 1e-4\n",
    "    x1, x2 = x_min + S, 1 - S\n",
    "    xmean = np.clip(xmean, x1, x2)\n",
    "    return (1 / level_gap) * np.clip(np.log(xmean - x_min) - np.log(1 - xmean), -300, 300)\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, initial_learning_rate=0.01, decay_rate=0.95, batch_size=10, n_iter=20, optimizer='sgd', cd_steps=1, spins=False, potts=False, l1_reg=0.0, init_method='glorot', n_jobs=-1, random_state=None):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.learning_rate = initial_learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.cd_steps = cd_steps\n",
    "        self.spins = spins\n",
    "        self.potts = potts\n",
    "        self.l1_reg = l1_reg\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.weights = self._initialize_weights(init_method)\n",
    "        self.visible_bias = np.zeros(n_visible)\n",
    "        self.hidden_bias = np.zeros(n_hidden)\n",
    "        self.opt = self._choose_optimizer()\n",
    "        self.learning_rate_history = []\n",
    "        self.log_likelihood_history = []\n",
    "        # Setup for spins/Potts/other configurations\n",
    "        if self.spins:\n",
    "            self.x_min = -1\n",
    "            self.level_gap = 2.0\n",
    "        else:\n",
    "            self.x_min = 0\n",
    "            self.level_gap = 1.0\n",
    "        \n",
    "        if self.potts:\n",
    "            self.str_simul = \"RBM_Potts\"\n",
    "            self.nz = self.n_hidden\n",
    "        else:\n",
    "            self.str_simul = \"RBM\"\n",
    "            self.nz = 2 ** self.n_hidden\n",
    "        \n",
    "        if self.potts and self.spins:\n",
    "            raise ValueError(\"POTTS and SPINS cannot coexist\")\n",
    "\n",
    "        # Set random seed if provided\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            tf.random.set_seed(self.random_state)\n",
    "\n",
    "    def _initialize_weights(self, method):\n",
    "        if method == 'xavier':\n",
    "            scale = np.sqrt(2.0 / (self.n_visible + self.n_hidden))\n",
    "        elif method == 'he':\n",
    "            scale = np.sqrt(2.0 / self.n_visible)\n",
    "        elif method == 'glorot':  ### GLOROT is default\n",
    "            scale = np.sqrt(4. / float(self.n_visible + self.n_hidden))\n",
    "        return np.random.randn(self.n_visible, self.n_hidden) * scale\n",
    "\n",
    "    def _choose_optimizer(self):\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.initial_learning_rate = 1.0\n",
    "            self.decay_rate = 0.95\n",
    "            return tf.optimizers.SGD(self.learning_rate)\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            self.initial_learning_rate = 0.05\n",
    "            self.decay_rate = 1.0\n",
    "            return tf.optimizers.RMSprop(self.learning_rate)\n",
    "        elif self.optimizer == 'adam':\n",
    "            self.initial_learning_rate = 0.001\n",
    "            self.decay_rate = 1.0\n",
    "            return tf.optimizers.Adam(self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer. Choose from 'sgd', 'rmsprop', 'adam'.\")\n",
    "\n",
    "    def _update_learning_rate(self, epoch):\n",
    "        self.learning_rate = self.initial_learning_rate * (self.decay_rate ** epoch)\n",
    "        self.opt.learning_rate.assign(self.learning_rate)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "\n",
    "    def _sample_prob(self, prob):\n",
    "        return np.random.binomial(1, prob)\n",
    "\n",
    "    def _l1_regularization(self):\n",
    "        return self.l1_reg * np.sum(np.abs(self.weights))\n",
    "\n",
    "    def contrastive_divergence(self, input_data):\n",
    "        positive_hidden_probs = np.nan_to_num(self._sigmoid(np.dot(input_data, self.weights) + self.hidden_bias), 0.0)\n",
    "        positive_hidden_activations = self._sample_prob(positive_hidden_probs)\n",
    "\n",
    "        for step in range(self.cd_steps):\n",
    "            negative_visible_probs = np.nan_to_num(self._sigmoid(np.dot(positive_hidden_activations, self.weights.T) + self.visible_bias), 0.0)\n",
    "            negative_visible_activations = self._sample_prob(negative_visible_probs)\n",
    "            negative_hidden_probs = np.nan_to_num(self._sigmoid(np.dot(negative_visible_activations, self.weights) + self.hidden_bias),0.0)\n",
    "            positive_hidden_activations = self._sample_prob(negative_hidden_probs)\n",
    "\n",
    "        return input_data, positive_hidden_probs, positive_hidden_activations, negative_visible_probs, negative_hidden_probs\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Initialize visible biases using Hinton's method\n",
    "        self.visible_bias = Hinton_bias_init(data, x_min=self.x_min, level_gap=self.level_gap)\n",
    "        for epoch in range(self.n_iter+1):\n",
    "            np.random.shuffle(data)\n",
    "            batches = [data[k:k + self.batch_size] for k in range(0, data.shape[0], self.batch_size)]\n",
    "            \n",
    "            # Update learning rate\n",
    "            self._update_learning_rate(epoch)\n",
    "            self.learning_rate_history.append(self.learning_rate)\n",
    "\n",
    "            for batch in batches:\n",
    "                input_data, positive_hidden_probs, positive_hidden_activations, negative_visible_probs, negative_hidden_probs = self.contrastive_divergence(batch)\n",
    "                \n",
    "                positive_grad = np.dot(input_data.T, positive_hidden_probs)\n",
    "                negative_grad = np.dot(negative_visible_probs.T, negative_hidden_probs)\n",
    "                \n",
    "                self.weights += self.learning_rate * (positive_grad - negative_grad) / self.batch_size\n",
    "                self.visible_bias += self.learning_rate * np.mean(input_data - negative_visible_probs, axis=0)\n",
    "                self.hidden_bias += self.learning_rate * np.mean(positive_hidden_probs - negative_hidden_probs, axis=0)\n",
    "\n",
    "                # Apply L2 and L1 regularization\n",
    "                self.weights -=  self._l1_regularization()\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                log_likelihood = self.compute_log_likelihood(data)\n",
    "                print(f\"Epoch {epoch + 1}/{self.n_iter}, Log-Likelihood: {log_likelihood:.2f}, lr = {self.opt.learning_rate.numpy():.2f}\")\n",
    "                self.log_likelihood_history.append(log_likelihood)\n",
    "\n",
    "    def transform(self, data):\n",
    "        hidden_probs = self._sigmoid(np.dot(data, self.weights) + self.hidden_bias)\n",
    "        return hidden_probs\n",
    "\n",
    "    def compute_log_likelihood(self, data):\n",
    "        visible_bias_term = np.dot(data, self.visible_bias)\n",
    "        hidden_bias_term = np.sum(np.log(1 + np.exp(np.clip(np.dot(data, self.weights) + self.hidden_bias, -700,+680))), axis=1)\n",
    "        log_likelihood = np.mean(visible_bias_term + hidden_bias_term)\n",
    "        return np.nan_to_num(log_likelihood,7)\n",
    "\n",
    "    def plot_learning_rate(self):\n",
    "        plt.plot(self.learning_rate_history)\n",
    "        plt.title(\"Learning Rate Over Time\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_log_likelihood(self):\n",
    "        with open(f'./llh_fig/llh_hist_L{self.n_hidden}_CD{self.cd_steps}_nepoch{self.n_iter}_grad{self.optimizer}.txt', \"w\") as file:\n",
    "            for item in self.log_likelihood_history:\n",
    "                file.write(str(item) + \"\\n\")\n",
    "        plt.plot(self.log_likelihood_history)\n",
    "        plt.title(\"Log-Likelihood Over Time\")\n",
    "        plt.xlabel(\"Epoch (every 5th epoch)\")\n",
    "        plt.ylabel(\"Log-Likelihood\")\n",
    "        \n",
    "        plt.savefig(f'./llh_fig/llh_hist_L{self.n_hidden}_CD{self.cd_steps}_nepoch{self.n_iter}_grad{self.optimizer}.png')\n",
    "        plt.show()\n",
    "\n",
    "    def cross_validate_with_seeds(self, data, seeds):\n",
    "        log_likelihoods = []\n",
    "        for seed in seeds:\n",
    "            # Set the random seed\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "            # Reinitialize the model with the same parameters but a different random seed\n",
    "            rbm = RBM(n_visible=self.n_visible, n_hidden=self.n_hidden, initial_learning_rate=self.initial_learning_rate,\n",
    "                      decay_rate=self.decay_rate, batch_size=self.batch_size, n_iter=self.n_iter, optimizer=self.optimizer,\n",
    "                      cd_steps=self.cd_steps, spins=self.spins, potts=self.potts, l1_reg=self.l1_reg,\n",
    "                      init_method='glorot', random_state=seed)\n",
    "            # Fit the model to the data\n",
    "            rbm.fit(data)\n",
    "            # Compute the log-likelihood\n",
    "            log_likelihood = rbm.compute_log_likelihood(data)\n",
    "            log_likelihoods.append(log_likelihood)\n",
    "            print(f\"Seed {seed}, Log-Likelihood: {log_likelihood:.2f}\")\n",
    "        mean_log_likelihood = np.mean(log_likelihoods)\n",
    "        std_log_likelihood = np.std(log_likelihoods)\n",
    "        print(f'Mean Log-Likelihood: {mean_log_likelihood:.2f}')\n",
    "        print(f'Standard Deviation of Log-Likelihood: {std_log_likelihood:.2f}')\n",
    "        return mean_log_likelihood, std_log_likelihood\n",
    "\n",
    "    def sample(self, n_samples, n_gibbs_steps=1000):\n",
    "        samples = np.random.rand(n_samples, self.n_visible) > 0.5\n",
    "        \n",
    "        for step in range(n_gibbs_steps):\n",
    "            hidden_probs = self._sigmoid(np.dot(samples, self.weights) + self.hidden_bias)\n",
    "            hidden_activations = self._sample_prob(hidden_probs)\n",
    "            visible_probs = self._sigmoid(np.dot(hidden_activations, self.weights.T) + self.visible_bias)\n",
    "            samples = self._sample_prob(visible_probs)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def plot_samples(self, samples, n_cols=10):\n",
    "        n_samples, n_features = samples.shape\n",
    "        n_rows = int(np.ceil(n_samples / n_cols))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < n_samples:\n",
    "                ax.imshow(samples[i].reshape(int(np.sqrt(n_features)), int(np.sqrt(n_features))), cmap=\"gray\")\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "def plot_weights_bias(rbm, epoch, L, \n",
    "                      side=0, cols=0, thr=0, s=1.5, \n",
    "                      title=False, save=True, cmap=\"bwr\"):\n",
    "    '''\n",
    "    Plot the weights of the RBM, one plot for each hidden unit.\n",
    "    '''\n",
    "    rows = int(np.ceil(L / cols))\n",
    "    if rows == 1: rows = 2\n",
    "    w = rbm.weights\n",
    "    b = rbm.visible_bias\n",
    "    if side == 0: side = int(np.sqrt(len(w)))\n",
    "    if thr == 0: thr = 4\n",
    "    plt.clf()\n",
    "    fig, AX = plt.subplots(rows, cols + 1, figsize=(s * (1 + cols), s * rows))\n",
    "    if title: fig.suptitle(f\"epoch = {epoch}\")\n",
    "    k = 1\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if rows == 1: ax = AX[j + 1]\n",
    "            else: ax = AX[i, j + 1]\n",
    "            if k <= L:\n",
    "                ax.imshow(w[:, k - 1].reshape(side, side), cmap=cmap, vmin=-thr, vmax=thr)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_title(f\"hidden {k}\")\n",
    "            else: fig.delaxes(ax)\n",
    "            k += 1\n",
    "        if i > 0:  fig.delaxes(AX[i, 0])\n",
    "    \n",
    "    ax = AX[0, 0]\n",
    "    im = ax.imshow(b.reshape(side, side), cmap=cmap, vmin=-thr, vmax=thr)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"bias\")\n",
    "    # colobar\n",
    "    cbar_ax = fig.add_axes([0.14, 0.15, 0.024, 0.33])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    \n",
    "    S = 0.3\n",
    "    plt.subplots_adjust(hspace=S)\n",
    "\n",
    "    if save: plt.savefig(f\"./FIG/FRAME/RBM_{epoch}_w-a.png\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "872e3014-5fc9-4a32-8819-64a171aa52a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gigi/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Log-Likelihood: 0.00, lr = 1.00\n",
      "Epoch 6/20, Log-Likelihood: 0.00, lr = 0.77\n",
      "Epoch 11/20, Log-Likelihood: 0.00, lr = 0.60\n",
      "Epoch 16/20, Log-Likelihood: 0.00, lr = 0.46\n",
      "Epoch 21/20, Log-Likelihood: 0.00, lr = 0.36\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_original, Y_original = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Parameters\n",
    "Ndigit = 3\n",
    "L = 3\n",
    "optimizer = 'sgd'\n",
    "\n",
    "# Select data\n",
    "list_10_digits = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "list_digits = list_10_digits[-Ndigit:]\n",
    "keep = np.isin(Y_original, list_digits)\n",
    "X_keep, Y = X_original[keep], Y_original[keep]\n",
    "\n",
    "# Binarize data\n",
    "binarizer = Binarizer(threshold=127.5)\n",
    "data = binarizer.fit_transform(X_keep)\n",
    "\n",
    "# Initialize and train the RBM\n",
    "rbm = RBM(n_visible=data.shape[1], n_hidden=L, initial_learning_rate=1., decay_rate=0.95, batch_size=10, n_iter=20, optimizer=optimizer, cd_steps=1, spins=False, potts=False, l1_reg=0.001, init_method='glorot')\n",
    "rbm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c4261-256d-4ecb-8f10-d19ae41da584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gigi/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Log-Likelihood: nan, lr = 1.00\n",
      "Epoch 6/20, Log-Likelihood: nan, lr = 0.77\n",
      "Epoch 11/20, Log-Likelihood: nan, lr = 0.60\n",
      "Epoch 16/20, Log-Likelihood: nan, lr = 0.46\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "p < 0, p > 1 or p contains NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#mean_log_likelihood, std_log_likelihood = rbm.cross_validate_with_seeds(data, seeds)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Generate new samples\u001b[39;00m\n\u001b[1;32m     28\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 29\u001b[0m samples \u001b[38;5;241m=\u001b[39m rbm\u001b[38;5;241m.\u001b[39msample(n_samples, n_gibbs_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Plot the generated samples\u001b[39;00m\n\u001b[1;32m     32\u001b[0m rbm\u001b[38;5;241m.\u001b[39mplot_samples(samples, n_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 204\u001b[0m, in \u001b[0;36mRBM.sample\u001b[0;34m(self, n_samples, n_gibbs_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_gibbs_steps):\n\u001b[1;32m    203\u001b[0m     hidden_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sigmoid(np\u001b[38;5;241m.\u001b[39mdot(samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_bias)\n\u001b[0;32m--> 204\u001b[0m     hidden_activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_prob(hidden_probs)\n\u001b[1;32m    205\u001b[0m     visible_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sigmoid(np\u001b[38;5;241m.\u001b[39mdot(hidden_activations, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisible_bias)\n\u001b[1;32m    206\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_prob(visible_probs)\n",
      "Cell \u001b[0;32mIn[28], line 101\u001b[0m, in \u001b[0;36mRBM._sample_prob\u001b[0;34m(self, prob)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, prob):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(\u001b[38;5;241m1\u001b[39m, prob)\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:3460\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.binomial\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:391\u001b[0m, in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:377\u001b[0m, in \u001b[0;36mnumpy.random._common._check_array_cons_bounded_0_1\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: p < 0, p > 1 or p contains NaNs"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation with different random seeds\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "#mean_log_likelihood, std_log_likelihood = rbm.cross_validate_with_seeds(data, seeds)\n",
    "\n",
    "# Generate new samples\n",
    "n_samples = 10\n",
    "samples = rbm.sample(n_samples, n_gibbs_steps=100)\n",
    "\n",
    "# Plot the generated samples\n",
    "rbm.plot_samples(samples, n_cols=10)\n",
    "\n",
    "# Plot the learning rate\n",
    "rbm.plot_learning_rate()\n",
    "\n",
    "# Plot the log-likelihood\n",
    "rbm.plot_log_likelihood()\n",
    "\n",
    "# Example usage\n",
    "plot_weights_bias(rbm, epoch=10, L=3, cols=3, title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02712377-5890-41a1-924f-24d39628bbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
