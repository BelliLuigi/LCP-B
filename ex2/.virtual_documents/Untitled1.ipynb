





import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import Binarizer
import tensorflow as tf

def Hinton_bias_init(x, x_min=0, level_gap=1):
    '''
    Initialize visible biases based on the average value in the dataset.
    Hinton, "A Practical Guide to Training Restricted Boltzmann Machines"
    x: data array
    x_min: minimum value of x
    level_gap: difference between max and min values in x
    '''
    xmean = np.mean(x, axis=0)
    S = 1e-4
    x1, x2 = x_min + S, 1 - S
    xmean = np.clip(xmean, x1, x2)
    return (1 / level_gap) * np.clip(np.log(xmean - x_min) - np.log(1 - xmean), -300, 300)

class RBM:
    def __init__(self, n_visible, n_hidden, initial_learning_rate=0.01, decay_rate=0.95, batch_size=10, n_iter=20, optimizer='sgd', cd_steps=1, spins=False, potts=False, n_jobs=-1):
        self.n_visible = n_visible
        self.n_hidden = n_hidden
        self.initial_learning_rate = initial_learning_rate
        self.learning_rate = initial_learning_rate
        self.decay_rate = decay_rate
        self.batch_size = batch_size
        self.n_iter = n_iter
        self.optimizer = optimizer.lower()
        self.cd_steps = cd_steps
        self.spins = spins
        self.potts = potts
        self.n_jobs = n_jobs
        self.weights = np.random.randn(n_visible, n_hidden) * 0.1
        self.visible_bias = np.zeros(n_visible)
        self.hidden_bias = np.zeros(n_hidden)
        self.opt = self._choose_optimizer()
        self.learning_rate_history = []
        self.log_likelihood_history = []

        # Setup for spins/Potts/other configurations
        if self.spins:
            self.x_min = -1
            self.level_gap = 2.0
        else:
            self.x_min = 0
            self.level_gap = 1.0
        
        if self.potts:
            self.str_simul = "RBM_Potts"
            self.nz = self.n_hidden
        else:
            self.str_simul = "RBM"
            self.nz = 2 ** self.n_hidden
        
        if self.potts and self.spins:
            raise ValueError("POTTS and SPINS cannot coexist")

    def _choose_optimizer(self):
        if self.optimizer == 'sgd':
            self.learning_rate = 1.0
            self.decay_rate = 0.95
            return tf.optimizers.SGD(self.learning_rate)
        elif self.optimizer == 'rmsprop':
            self.learning_rate = 0.05
            self.decay_rate = 1
            return tf.optimizers.RMSprop(self.learning_rate)
        elif self.optimizer == 'adam':
            self.learning_rate = 0.001
            self.decay_rate = 1
            return tf.optimizers.Adam(self.learning_rate)
        else:
            raise ValueError("Unsupported optimizer. Choose from 'sgd', 'rmsprop', 'adam'.")

    def _update_learning_rate(self, epoch):
        self.learning_rate = self.initial_learning_rate * (self.decay_rate ** epoch)
        self.opt.learning_rate.assign(self.learning_rate)

    def _sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def _sample_prob(self, prob):
        return np.random.binomial(1, prob)

    def contrastive_divergence(self, input_data):
        positive_hidden_probs = self._sigmoid(np.dot(input_data, self.weights) + self.hidden_bias)
        positive_hidden_activations = self._sample_prob(positive_hidden_probs)

        for step in range(self.cd_steps):
            negative_visible_probs = self._sigmoid(np.dot(positive_hidden_activations, self.weights.T) + self.visible_bias)
            negative_visible_activations = self._sample_prob(negative_visible_probs)
            negative_hidden_probs = self._sigmoid(np.dot(negative_visible_activations, self.weights) + self.hidden_bias)
            positive_hidden_activations = self._sample_prob(negative_hidden_probs)

        return input_data, positive_hidden_probs, positive_hidden_activations, negative_visible_probs, negative_hidden_probs

    def fit(self, data):
        # Initialize visible biases using Hinton's method
        self.visible_bias = Hinton_bias_init(data, x_min=self.x_min, level_gap=self.level_gap)
        for epoch in range(self.n_iter):
            np.random.shuffle(data)
            batches = [data[k:k + self.batch_size] for k in range(0, data.shape[0], self.batch_size)]
            
            # Update learning rate
            self._update_learning_rate(epoch)
            self.learning_rate_history.append(self.learning_rate)

            for batch in batches:
                input_data, positive_hidden_probs, positive_hidden_activations, negative_visible_probs, negative_hidden_probs = self.contrastive_divergence(batch)
                
                positive_grad = np.dot(input_data.T, positive_hidden_probs)
                negative_grad = np.dot(negative_visible_probs.T, negative_hidden_probs)
                
                self.weights += self.learning_rate * (positive_grad - negative_grad) / self.batch_size
                self.visible_bias += self.learning_rate * np.mean(input_data - negative_visible_probs, axis=0)
                self.hidden_bias += self.learning_rate * np.mean(positive_hidden_probs - negative_hidden_probs, axis=0)
            log_likelihood = self.compute_log_likelihood(data)
            print(f"Epoch {epoch + 1}/{self.n_iter}, Log-Likelihood: {log_likelihood}, lr = {self.opt.learning_rate.numpy()}")
            self.log_likelihood_history.append(log_likelihood)

    def transform(self, data):
        hidden_probs = self._sigmoid(np.dot(data, self.weights) + self.hidden_bias)
        return hidden_probs

    def compute_log_likelihood(self, data):
        visible_bias_term = np.dot(data, self.visible_bias)
        hidden_bias_term = np.sum(np.log(1 + np.exp(np.dot(data, self.weights) + self.hidden_bias)), axis=1)
        log_likelihood = np.mean(visible_bias_term + hidden_bias_term)
        return log_likelihood

    def plot_learning_rate(self):
        plt.plot(self.learning_rate_history)
        plt.title("Learning Rate Over Time")
        plt.xlabel("Epoch")
        plt.ylabel("Learning Rate")
        plt.show()

    def plot_log_likelihood(self):
        plt.plot(self.log_likelihood_history)
        plt.title("Log-Likelihood Over Time")
        plt.xlabel("Epoch")
        plt.ylabel("Log-Likelihood")
        plt.show()

        
def plot_weights_bias(rbm, epoch, L, 
                      side=0, cols=0, thr=0, s=1.5, 
                      title=False, save=True, cmap="bwr"):
    '''
    Plot the weights of the RBM, one plot for each hidden unit.
    '''
    rows = int(np.ceil(L / cols))
    if rows == 1: rows = 2
    w = rbm.weights
    b = rbm.visible_bias
    if side == 0: side = int(np.sqrt(len(w)))
    if thr == 0: thr = 4
    plt.clf()
    fig, AX = plt.subplots(rows, cols + 1, figsize=(s * (1 + cols), s * rows))
    if title: fig.suptitle(f"epoch = {epoch}")
    k = 1
    for i in range(rows):
        for j in range(cols):
            if rows == 1: ax = AX[j + 1]
            else: ax = AX[i, j + 1]
            if k <= L:
                ax.imshow(w[:, k - 1].reshape(side, side), cmap=cmap, vmin=-thr, vmax=thr)
                ax.set_xticks([])
                ax.set_yticks([])
                ax.set_title(f"hidden {k}")
            else: fig.delaxes(ax)
            k += 1
        if i > 0:  fig.delaxes(AX[i, 0])
    
    ax = AX[0, 0]
    im = ax.imshow(b.reshape(side, side), cmap=cmap, vmin=-thr, vmax=thr)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title("bias")
    # colobar
    cbar_ax = fig.add_axes([0.14, 0.15, 0.024, 0.33])
    cbar = fig.colorbar(im, cax=cbar_ax)
    cbar.ax.tick_params(labelsize=12)
    
    S = 0.3
    plt.subplots_adjust(hspace=S)

    if save: plt.savefig(f"./FIG/FRAME/RBM_{epoch}_w-a.png")

    plt.show()
    plt.close()


# Load data
X_original, Y_original = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)


# Parameters
Ndigit = 3
L = 3
optimizer = 'SGD'
cd_steps=1

# Select data
list_10_digits = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')
list_digits = list_10_digits[-Ndigit:]
keep = np.isin(Y_original, list_digits)
X_keep, Y = X_original[keep], Y_original[keep]

# Binarize data
binarizer = Binarizer(threshold=127.5)
data = binarizer.fit_transform(X_keep)

# Initialize and train the RBM
for cd_steps in np.arange(1,20,2):
    rbm = RBM(n_visible=data.shape[1], n_hidden=L, batch_size=10, n_iter=60, optimizer=optimizer, cd_steps=cd_steps, spins=False, potts=False)
    rbm.fit(data)
    # Plot the learning rate
    #rbm.plot_learning_rate()
    # Plot the log-likelihood
    rbm.plot_log_likelihood()

    






